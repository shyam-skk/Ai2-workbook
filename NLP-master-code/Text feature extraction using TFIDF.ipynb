{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text feature extraction using TF-IDF\n",
    "\n",
    "In this section I'll use sklearn to build the training and testing data. Then I'll build the NN model and use TF-IDF methods to improve overall model accuracy.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and split into **X** train and **y** test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tsv file into a dataframe object\n",
    "# Press tab to check you are in the correct folder location and to browse\n",
    "# to the tsv file\n",
    "# The sep command indicates this files is separated by tabs\n",
    "dataframe = pd.read_csv(\"../Jupyter notebook files/SMSSpamCollection/SMSSpamCollection.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how many **spam** and **ham** we have in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 5572 messages, and 4824 are labelled as **ham**. \n",
    "\n",
    "Therefore 4824 / 5572 = 86% of messages are **ham**. If we were to randomly choose either **ham** or **spam** when viewing a text message, then 86% would randomly be correct with this dataset. So our text classifier needs to perform better than 86% to beat a random selection.\n",
    "\n",
    "First we split the data into **text message data** which we call **X** and **label data** which we call **y**. Make sure you pay attention to capitalisation convention rules for both datasets. **X** contains a large amount of message data and is therefore represneted with a capitalised letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following convention, X contains message data (large matrix)\n",
    "# and y contains label data\n",
    "X = dataframe[\"message\"]\n",
    "y = dataframe[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contains index and message text\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "3     ham\n",
       "4     ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contains index and message label\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train & test sets:\n",
    "\n",
    "Next we perform the training and testing split of the data, just as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test size represents the proportion of training and testing data split. \n",
    "# Random_state sets \"randomness\" of data randomisation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn's CountVectorizer\n",
    "\n",
    "The count vectorizer builds a dictionary of features and transforms documents to feature vectors.\n",
    "\n",
    "Text preprocessing, tokenising and the ability to filter out stopwords are all included in the count vectorizer in Scikit-learn. See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html for more detail.\n",
    "\n",
    "Stop words such as **and**, **the**, **him**, are uninformative in representing the content of a text and are removed to avoid them being construed as signal for prediction. See https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of count vectorizer\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to pass the **X_train** data into a count vectorizer and then transform the message data. Remember that X_train contains training message text data.\n",
    "\n",
    "There are 2 ways to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit count vectorizer to message data\n",
    "# This step builds vocabulary and counts number of words in text\n",
    "#count_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform original text message to a vector\n",
    "#X_train_count = count_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an easier method to complete this process using the `fit_transform` in the `CountVectorizer` class. This method performs the `fit` and the `transform` processes within one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative is to use the fit_transform function\n",
    "# which performs the fit and then transforms X_train\n",
    "# into a numerical vector and stores in X_train_counts\n",
    "# No need to do this in 2 separate functions (above)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the fit transform is a compressed sparse matrix. A sparse matrix is one in which most of the elements are zero. See https://en.wikipedia.org/wiki/Sparse_matrix for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets examine the dimensions of the sparse matrix. It contains 3900 rows of message data. Remember that previously we decided to split the message data into a 70/30 for training and testing. 70% of the original message texts (5572) = 3900. \n",
    "\n",
    "> The sparse matrix contains 7155 columns or features. These features represent the number of unique words in all of the text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3900x7155 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51338 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The matrix contains 3900 rows of text. These aare 70% of the original\n",
    "# text messages (5572 rows)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as size of no of rows in X_train\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform count vectorization to frequencies with Tf-idf\n",
    "While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might have the same tpoic content within them.\n",
    "\n",
    "To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called **Term frequeny** or **tf**.\n",
    "\n",
    "Another refinement on top of **tf** is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called **Term Frequency Inverse Document Frequency** or **tf–idf**.\n",
    "\n",
    "Both **tf** and **tf–idf** can be computed using scikit-learn's [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 7155)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Create an instance of TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Perform a tf-idf fit transform on the X_train_counts\n",
    "# sparse matrix. Put the result into X_train_tfidf\n",
    "X_train_transform = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Shape is the same as original count vectorizer\n",
    "# although it now contains word term frequencies multiplied by the\n",
    "# inverse document frequency\n",
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a library in scikit-learn that allows us to combine the processes of `count_vectorizer` and `fit_transform` into one process. It is called the `TfidfVectorizer`. This replaces the two steps of **vectorizing** followed by **fit transformation** processes that I did above.\n",
    "\n",
    "Here's how we can complete this work in one process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vectorizer library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 7155)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete the vectorizing and fit transform on the original X_train\n",
    "# dataset\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# Examine shape of the dataset\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lecture I compared accuracy of several text classifiers. I found that the **Support Vector Classifier (SVC)** performed best out of these. \n",
    "\n",
    "Now I'm going to apply a SVC called linearSVC. See https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC for more inforamtion.\n",
    "\n",
    "LinearSVC has more flexibility than SVC and should scale better to large numbers of samples. It can handle sparse matrix better then SVC and works well with large datasets.\n",
    "\n",
    "Let's set up `linearSVC` and compare its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078                         Yep, by the pretty sculpture\n",
       "4028        Yes, princess. Are you going to make me moan?\n",
       "958                            Welp apparently he retired\n",
       "4642                                              Havent.\n",
       "4674    I forgot 2 ask ü all smth.. There's a card on ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contents of X_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create an instance of the Linear SVC classifier and fit the **vectorized** and **fitted** X_train message data along with the y_train label data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the LinearSVC classifier\n",
    "classifier = LinearSVC()\n",
    "\n",
    "# X : {array-like, sparse matrix}\n",
    "# y : array-like, shape = [n_samples], \n",
    "# Target vector relative to X\n",
    "classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to prepare the testing data before it can be used for predictions from the SVC model. **Note** that I don't perform a `fit_transform` on the **X_text** data, just a `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform original test message data to a vector\n",
    "# No need to fit and transform it\n",
    "X_test_transform = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Predict message type from Linear SVC classifier\n",
    "predictions = classifier.predict(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions contains the predicted label data from inputted message test data\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1437    5]\n",
      " [  20  210]]\n"
     ]
    }
   ],
   "source": [
    "# Show a confusion matrix of results\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1442\n",
      "        spam       0.98      0.91      0.94       230\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1672\n",
      "   macro avg       0.98      0.95      0.97      1672\n",
      "weighted avg       0.98      0.99      0.98      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to test the new system with some typical HAM and SPAM text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham']\n"
     ]
    }
   ],
   "source": [
    "# Typical ham text message\n",
    "sample_text_message = [\"I'm going to go to work soon\"]\n",
    "transformed_text = tfidf_vectorizer.transform(sample_text_message)\n",
    "\n",
    "model_output = classifier.predict(transformed_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam']\n"
     ]
    }
   ],
   "source": [
    "# Typical spam text message\n",
    "sample_text_message = [\"You can win a holiday! Text 23455 to take this offer up.\"]\n",
    "transformed_text = tfidf_vectorizer.transform(sample_text_message)\n",
    "\n",
    "model_output = classifier.predict(transformed_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to create a **function** to simplify this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_type(text_message):\n",
    "    # Note that I'm using square brackets around the text_message variable\n",
    "    transformed_message = tfidf_vectorizer.transform([text_message])\n",
    "    \n",
    "    # Predict model output from \n",
    "    model_output = classifier.predict(transformed_message)\n",
    "    return(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit a text message to the function and see if the model can accurately detect whether it is ham or spam.\n",
    "# Here I'm testing whether the model can predict this as SPAM\n",
    "predict_message_type(\"You can win a holiday! Text 23455 to take this offer up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typical spam message\n",
    "predict_message_type(\"Your invoice is attached to this text. Click this link to download it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_message_type(\"Hi there. Hows things with you today? Are you heading out for some food?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a  scikit-learn Pipeline\n",
    "\n",
    "So far I've had to vectorize the training data, then build a Linear SVC classifier, fit the model with training data, and then transform the testing data before it can be used for predictions from the trained model.\n",
    "\n",
    "And I've then tried to speed up the process by implementing a function to complete some of the repetitiveness actiosn required to test the model.\n",
    "\n",
    "Fortunately scikit-learn offers a [**Pipeline**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class that behaves like a compound classifier. This means we can creat a `pipeline` to contain several libraries that can be called just like a method.\n",
    "\n",
    "Let's use a pipeline to complete the processes I just did on the Linear SVC classifier. Note that these steps will produce the same results as the steps I completed above.\n",
    "\n",
    "When `text_classifier.fit(X_train, y_train)` is called, the pipeline takes in **X_train** data and performs the `TfidfVectorizer` on it. A model is then stored in memory.\n",
    "\n",
    "When `predictions = text_classifier.predict(X_test)` is called, **X_text** is first transformed  with `tfidf_vect` and then the `LinearSVC_classifier` is used to predict using **X_test** data.  All steps are performed within the `text_classifier` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf_vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# If we didn't have these libraries loaded then we should\n",
    "# also do this process\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "text_classifier = Pipeline([('tfidf_vect', TfidfVectorizer()),\n",
    "                     ('LinearSVC_classifier', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1437    5]\n",
      " [  20  210]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1442\n",
      "        spam       0.98      0.91      0.94       230\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1672\n",
      "   macro avg       0.98      0.95      0.97      1672\n",
      "weighted avg       0.98      0.99      0.98      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850478468899522\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can predict whether a text message is **spam** or **ham** correctly in 98.5% of time. This shows a large improvement over the previous data we used for predictions.\n",
    "\n",
    "We can test the model out by feeding in some examples of a ham and spam messages. Remember **ham** is a genuine text message whereas **spam** is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Typical ham text message\n",
    "text_classifier.predict([\"I'm going to go to work soon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.predict([\"This is my phone number. I hope you can call before 4pm.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.predict([\"You can win a holiday! Text 23455 to take this offer up.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier.predict([\"Your invoice is attached to this text. Click this link to download it.\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
