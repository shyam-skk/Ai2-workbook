{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "So far we've seen how text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
    "\n",
    "In this section we will identify and label specific tokens and phrases that match patterns we can define ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules-based Matching\n",
    "\n",
    "spaCy’s rule-based matcher engines and components not only let you find you the words and phrases you’re looking for – they also give you access to the tokens within the document and their relationships. This means you can easily access and analyse the surrounding tokens, merge spans into single tokens or add entries to the named entities in `doc.ents`.\n",
    "\n",
    "spaCy offers a **rule-matching tool** called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. \n",
    "\n",
    "We can match on any part of the token including text and annotations, and web add multiple patterns to the same matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a token pattern\n",
    "\n",
    "For this example, I want to find three combinations of the words **stop word**. The three combinations of these words are:\n",
    "\n",
    "(a) a token that looks for lowercase text **stopword**<br>\n",
    "(b) a token where the `is_punct` flag is set to `True` so that any punctuation is detected eg **stop-word**<br>\n",
    "(c) a token where two words are found that read **stop** and **word** with a space in between eg **stop word**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the `matcher` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matcher library\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create each pattern. There are several token attributes we can use. These are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<thead><tr class=\"_8a68569b\"><th class=\"_2e8d2972\">Attribute</th><th class=\"_2e8d2972\">Type</th><th class=\"_2e8d2972\">&nbsp;Description</th></tr></thead>\n",
    "<tbody><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">ORTH</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The exact verbatim text of a token.</td>\n",
    "    </tr>\n",
    "    <tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">LOWER</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The lowercase form of the token text.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">LENGTH</code></td><td class=\"_5c99da9a\">int</td><td class=\"_5c99da9a\">The length of the token text.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">IS_ALPHA</code>, <code class=\"_1d7c6046\">IS_ASCII</code>, <code class=\"_1d7c6046\">IS_DIGIT</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token text consists of alphabetic characters, ASCII characters, digits.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">IS_LOWER</code>, <code class=\"_1d7c6046\">IS_UPPER</code>, <code class=\"_1d7c6046\">IS_TITLE</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token text is in lowercase, uppercase, titlecase.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">IS_PUNCT</code>, <code class=\"_1d7c6046\">IS_SPACE</code>, <code class=\"_1d7c6046\">IS_STOP</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token is punctuation, whitespace, stop word.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">LIKE_NUM</code>, <code class=\"_1d7c6046\">LIKE_URL</code>, <code class=\"_1d7c6046\">LIKE_EMAIL</code></td><td class=\"_5c99da9a\">bool</td><td class=\"_5c99da9a\">Token text resembles a number, URL, email.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\">&nbsp;<code class=\"_1d7c6046\">POS</code>, <code class=\"_1d7c6046\">TAG</code>, <code class=\"_1d7c6046\">DEP</code>, <code class=\"_1d7c6046\">LEMMA</code>, <code class=\"_1d7c6046\">SHAPE</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The token’s simple and extended part-of-speech tag, dependency label, lemma, shape.</td></tr><tr class=\"_8a68569b\"><td class=\"_5c99da9a\"><code class=\"_1d7c6046\">ENT_TYPE</code></td><td class=\"_5c99da9a\">unicode</td><td class=\"_5c99da9a\">The token’s entity label.</td></tr>\n",
    "    </tbody>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the three matching tokens for the three combinations of **stop word** described above. Note that we don't need to tokenise a single space as it is not recognised as punctuation.\n",
    "\n",
    "It doesn't matter if the attribute names are upper or lowercase. spaCy will normalise the names internally and `{\"LOWER\": \"text\"}` and `{\"lower\": \"text\"}` will both produce the same result. Using the uppercase version is mostly a convention to make it clear that the attributes are **special** and don’t exactly map to the token attributes like `Token.lower` and `Token.lower_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match for \"stopword\"\n",
    "token_match1 = [{\"LOWER\": \"stopword\"}]\n",
    "# match for \"stopwords\"\n",
    "token_match2 = [{\"LOWER\": \"stopwords\"}]\n",
    "# match for stop-word\n",
    "token_match3 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"word\"}]\n",
    "# match for stop-words\n",
    "token_match4 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"words\"}]\n",
    "# match for \"stop word\". We don't need to check for a single space as it is not tokenised\n",
    "token_match5 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"word\"}]\n",
    "# stopwords\n",
    "token_match6 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"words\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call `matcher.add` command to add all three token matches. The second argument lets you pass in an optional callback function to invoke on a successful match. For now, we set it to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(\"StopWord\", None, token_match1, token_match2, token_match3, token_match4, token_match5, token_match6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the matcher to a doc object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = open(\"Stop words.txt\")\n",
    "sentence = file_name.read()\n",
    "doc_object = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words like \"a\" and \"the\" are called stop---words.\n",
      "Sometimes this can be written as stop-words or stopwords.\n",
      "Each stop word can be filtered from the text to be processed.\n",
      "spaCy holds a built-in list of some 305 English stop--words.\n"
     ]
    }
   ],
   "source": [
    "print(doc_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_matches = matcher(doc_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17470060577089942448, 11, 14)\n",
      "(17470060577089942448, 22, 25)\n",
      "(17470060577089942448, 26, 27)\n",
      "(17470060577089942448, 30, 32)\n",
      "(17470060577089942448, 54, 57)\n"
     ]
    }
   ],
   "source": [
    "for token in token_matches:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function that accepts a string and displays the matcher objects. I'll also structure the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(text):\n",
    "    # convert text to a doc object\n",
    "    doc_object = nlp(text)\n",
    "    print(doc_object)\n",
    "    # find all matches within the doc object\n",
    "    token_matches = matcher(doc_object)\n",
    "    # For each item in the token_matches provide the following\n",
    "    # match_id is the hash value of the identified token match\n",
    "    for match_id, start, end in token_matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        matched_span = doc_object[start:end]      \n",
    "        print(f\"{match_id:<{20}} {string_id:<{15}} {start:{3}} {end:{3}} {matched_span.text:{20}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll send in the text from the earlier example into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words like \"a\" and \"the\" are called stop---words.\n",
      "Sometimes this can be written as stop-words or stopwords.\n",
      "Each stop word can be filtered from the text to be processed.\n",
      "spaCy holds a built-in list of some 305 English stop--words.\n",
      "17470060577089942448 StopWord         11  14 stop---words        \n",
      "17470060577089942448 StopWord         22  25 stop-words          \n",
      "17470060577089942448 StopWord         26  27 stopwords           \n",
      "17470060577089942448 StopWord         30  32 stop word           \n",
      "17470060577089942448 StopWord         54  57 stop--words         \n"
     ]
    }
   ],
   "source": [
    "find_matches(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting pattern options and quantifiers\n",
    "\n",
    "The following quantifiers can be passed to the `'OP'` key:\n",
    "<table><tr><th>OP</th><th>Description</th></tr>\n",
    "\n",
    "<tr ><td><span >\\!</span></td><td>Negate the pattern, by requiring it to match exactly 0 times</td></tr>\n",
    "<tr ><td><span >?</span></td><td>Make the pattern optional, by allowing it to match 0 or 1 times</td></tr>\n",
    "<tr ><td><span >\\+</span></td><td>Require the pattern to match 1 or more times</td></tr>\n",
    "<tr ><td><span >\\*</span></td><td>Allow the pattern to match zero or more times</td></tr>\n",
    "</table>\n",
    "\n",
    "You can make token rules optional by passing an `'OP':'*'` argument.  \n",
    "\n",
    "This lets us streamline our patterns list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old matcher to avoid issues\n",
    "matcher.remove(\"StopWord\")\n",
    "\n",
    "# Redefine the patterns:\n",
    "token_match1 = [{\"LOWER\": \"stopword\"}]\n",
    "token_match2 = [{\"LOWER\": \"stopwords\"}]\n",
    "token_match3 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True, \"OP\":\"*\"}, {\"LOWER\": \"word\"}]\n",
    "token_match4 = [{\"LOWER\": \"stop\"}, {\"IS_PUNCT\": True, \"OP\":\"*\"}, {\"LOWER\": \"words\"}]\n",
    "token_match5 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"word\"}]\n",
    "token_match6 = [{\"LOWER\": \"stop\"}, {\"LOWER\": \"words\"}]\n",
    "\n",
    "# Add the new set of patterns to the 'SolarPower' matcher:\n",
    "matcher.add(\"StopWord\", None, token_match1, token_match2, token_match3, token_match4, token_match5, token_match6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words like \"a\" and \"the\" are called stop---words.\n",
      "Sometimes this can be written as stop-words or stopwords.\n",
      "Each stop word can be filtered from the text to be processed.\n",
      "spaCy holds a built-in list of some 305 English stop--words.\n",
      "17470060577089942448 StopWord         11  14 stop---words        \n",
      "17470060577089942448 StopWord         22  25 stop-words          \n",
      "17470060577089942448 StopWord         26  27 stopwords           \n",
      "17470060577089942448 StopWord         30  32 stop word           \n",
      "17470060577089942448 StopWord         30  32 stop word           \n",
      "17470060577089942448 StopWord         54  57 stop--words         \n"
     ]
    }
   ],
   "source": [
    "file_name = open(\"Stop words.txt\")\n",
    "sentence = file_name.read()\n",
    "#my_text = \"Words like \\\"a\\\" and \\\"the\\\" are called stop---words.\\\n",
    "#Sometimes this can be written as stop-words or stopwords.\\\n",
    "#Each stop word can be filtered from the text to be processed.\\\n",
    "#spaCy holds a built-in list of some 305 English stop--words.\"\n",
    "\n",
    "find_matches(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be careful with lemmatisation searching\n",
    "If we wanted to match on the words '**petrol power** and **petrol powered**, it might be tempting to look for the **lemma** of **powered** and expect it to be **power**. Then we could potentially pick that up with a **lemmatisation** match. This is not always the case though. The lemma of the adjective **powered** is still **powered**.\n",
    "\n",
    "Lets look at an example of this problem.\n",
    "\n",
    "First I'll create an exemplar sentence and show the lemmas from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petrol\t -----> petrol\tPROPN\n",
      "-\t -----> -\tPUNCT\n",
      "powered\t -----> power\tVERB\n",
      "energy\t -----> energy\tNOUN\n",
      "runs\t -----> run\tVERB\n",
      "petrol\t -----> petrol\tNOUN\n",
      "-\t -----> -\tPUNCT\n",
      "powered\t -----> powered\tADJ\n",
      "cars\t -----> car\tNOUN\n",
      ".\t -----> .\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "doc_object = nlp(u\"Petrol-powered energy runs petrol-powered cars.\")\n",
    "\n",
    "# Lets look at the lemmatisation of each word\n",
    "for word in doc_object:\n",
    "    print (word.text + \"\\t\" + \" -----> \" + word.lemma_ + \"\\t\" + word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second **powered** word is an adjective so it can't match on the lemma **power** since an adjective does not reduced down to the base word **power**. This example will not work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_match1 = [{'LOWER': 'petrolpower'}]\n",
    "token_match2 = [{'LOWER': 'petrol'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}]\n",
    "\n",
    "# Add the new set of patterns to the 'SolarPower' matcher:\n",
    "matcher.add('PetrolPower', None, token_match1, token_match2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15516410614135709684, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc_object)\n",
    "print (found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the first occurrence of **petrol-powered** is recognised. The second occurrence's lemma equivelant does not change to **power** so it is not matched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Matcher\n",
    "In token-based matching we used token patterns to perform rule-based matching. \n",
    "\n",
    "An alternative - and often more efficient method is to match on terminology lists. In this case we use `PhraseMatcher` to create a Doc object from a list of phrases, and pass that into `matcher` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PhraseMatcher library\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example text is from this link https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "    \n",
    "It is also available on Blackboard under the file name **NLP.txt**.\n",
    "\n",
    "Before opening the text file, make sure it is in the same folder location as your jupyter notebook file. You should be able to use predictive text to pick up the name of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Jupyter notebook files/NLP.txt\", encoding = \"utf8\") as my_file:\n",
    "    doc_object = nlp(my_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I would like to match on some words within the text file I've just imported. \n",
    "\n",
    "I've created a list of match phrases I would like to check my imported text for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[natural language processing, machine learning, supervised learning, machine translation]\n"
     ]
    }
   ],
   "source": [
    "phrase_list = [\"natural language processing\", \"machine learning\", \"supervised learning\", \"machine translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will convert each of these phrases into a suitable structure. I'm going to create a `doc` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, convert each phrase to a Doc object:\n",
    "phrase_patterns = [nlp.make_doc(word) for word in phrase_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at these phrase patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[natural language processing, machine learning, supervised learning, machine translation]\n"
     ]
    }
   ],
   "source": [
    "# Show these phrase patterns\n",
    "print(phrase_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll add each of these phrase patterns to a `matcher` object called **NLP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass each Doc object into matcher (note the use of the asterisk)\n",
    "# refers to a *phrase_patterns (Doc): `Doc` objects representing match patterns.\n",
    "matcher.add(\"NLP\", None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I'll build a list of relevant matches and put the results into a variable called **matches**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of matches:\n",
    "matches = matcher(doc_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the contents of the found matches. Each match contains the `match_id`, and the `start` and `stop` locations of each match within the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15832915187156881108, 3, 6),\n",
       " (15832915187156881108, 85, 87),\n",
       " (15832915187156881108, 127, 129),\n",
       " (15832915187156881108, 137, 139),\n",
       " (15832915187156881108, 150, 152),\n",
       " (15832915187156881108, 160, 163),\n",
       " (15832915187156881108, 368, 371),\n",
       " (15832915187156881108, 396, 399),\n",
       " (15832915187156881108, 403, 405),\n",
       " (15832915187156881108, 471, 473),\n",
       " (15832915187156881108, 512, 515),\n",
       " (15832915187156881108, 622, 624),\n",
       " (15832915187156881108, 762, 764),\n",
       " (15832915187156881108, 807, 809),\n",
       " (15832915187156881108, 890, 892),\n",
       " (15832915187156881108, 896, 899),\n",
       " (15832915187156881108, 1037, 1040),\n",
       " (15832915187156881108, 1047, 1049),\n",
       " (15832915187156881108, 1062, 1064),\n",
       " (15832915187156881108, 1091, 1093)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show each match using a loop I created earlier in this document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15832915187156881108 \t NLP \t 3 \t 6 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 85 \t 87 \t machine translation\n",
      "15832915187156881108 \t NLP \t 127 \t 129 \t machine translation\n",
      "15832915187156881108 \t NLP \t 137 \t 139 \t machine translation\n",
      "15832915187156881108 \t NLP \t 150 \t 152 \t machine translation\n",
      "15832915187156881108 \t NLP \t 160 \t 163 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 368 \t 371 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 396 \t 399 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 403 \t 405 \t machine learning\n",
      "15832915187156881108 \t NLP \t 471 \t 473 \t machine learning\n",
      "15832915187156881108 \t NLP \t 512 \t 515 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 622 \t 624 \t machine translation\n",
      "15832915187156881108 \t NLP \t 762 \t 764 \t supervised learning\n",
      "15832915187156881108 \t NLP \t 807 \t 809 \t supervised learning\n",
      "15832915187156881108 \t NLP \t 890 \t 892 \t machine learning\n",
      "15832915187156881108 \t NLP \t 896 \t 899 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 1037 \t 1040 \t natural language processing\n",
      "15832915187156881108 \t NLP \t 1047 \t 1049 \t machine translation\n",
      "15832915187156881108 \t NLP \t 1062 \t 1064 \t machine translation\n",
      "15832915187156881108 \t NLP \t 1091 \t 1093 \t machine translation\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc_object[start:end]\n",
    "    print(match_id, \"\\t\", string_id, \"\\t\", start, \"\\t\", end, \"\\t\", span.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Matches\n",
    "There are a few ways to fetch the text surrounding a match. The simplest is to grab a slice of tokens from the doc object that is wider than the match.\n",
    "\n",
    "For example, the first **machine translation** match occurs between words 85 - 86. I can view the context of the sentence it is in by choosing a few wordseither side of its location within the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "three or five years, machine translation would be a solved problem.[2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allowing a few words either side of the match\n",
    "doc_object[80:93]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the loop I created earlier to capture some text on either side of the matched phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP \t 3 \t 6 \t The history of natural language processing (NLP)\n",
      "NLP \t 85 \t 87 \t five years, machine translation would be a\n",
      "NLP \t 127 \t 129 \t , funding for machine translation was dramatically reduced\n",
      "NLP \t 137 \t 139 \t further research in machine translation was conducted until\n",
      "NLP \t 150 \t 152 \t the first statistical machine translation systems were developed\n",
      "NLP \t 160 \t 163 \t Some notably successful natural language processing systems developed in\n",
      "NLP \t 368 \t 371 \t 1980s, most natural language processing systems were based\n",
      "NLP \t 396 \t 399 \t a revolution in natural language processing with the introduction\n",
      "NLP \t 403 \t 405 \t the introduction of machine learning algorithms for language\n",
      "NLP \t 471 \t 473 \t earliest-used machine learning algorithms, such\n",
      "NLP \t 512 \t 515 \t Markov models to natural language processing, and increasingly\n",
      "NLP \t 622 \t 624 \t the field of machine translation, due especially\n",
      "NLP \t 762 \t 764 \t and semi-supervised learning algorithms. Such\n",
      "NLP \t 807 \t 809 \t more difficult than supervised learning, and typically\n",
      "NLP \t 890 \t 892 \t network-style machine learning methods became widespread\n",
      "NLP \t 896 \t 899 \t became widespread in natural language processing, due in\n",
      "NLP \t 1037 \t 1040 \t distinct from statistical natural language processing. For instance\n",
      "NLP \t 1047 \t 1049 \t the term neural machine translation (NMT)\n",
      "NLP \t 1062 \t 1064 \t based approaches to machine translation directly learn sequence\n",
      "NLP \t 1091 \t 1093 \t used in statistical machine translation (SMT)\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc_object[start-3:end+3]\n",
    "    print(string_id, \"\\t\", start, \"\\t\", end, \"\\t\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to first apply the `sentencizer` to the doc object, then iterate through the sentences to the match point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24\n"
     ]
    }
   ],
   "source": [
    "# Build a list of sentences\n",
    "sentences = [sent for sent in doc_object.sents]\n",
    "\n",
    "# Sentences contain start and end token values\n",
    "# for example, here's the start and end values of the first sentence\n",
    "print(sentences[0].start, sentences[0].end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. 93 133 129\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the sentence list until the sentence end value exceeds a match start value:\n",
    "for sent in sentences:\n",
    "    # matches[2][2] refers to the 3rd row in matches and the third column \"129\"\n",
    "    # send.end is the end of an occurrence of \"sent\"\n",
    "    if matches[2][2] < sent.end:\n",
    "        print(sent, sent.start, sent.end, matches[2][2])\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
