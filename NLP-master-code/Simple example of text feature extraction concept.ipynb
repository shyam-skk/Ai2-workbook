{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Natural Language Processor manually\n",
    "\n",
    "In this section I'll use basic Python to build a rudimentary NLP system. We'll build a **corpus of documents** (two small lists of sentences), create a **vocabulary** from all the words in both documents, and then demonstrate a *Bag of Words* technique to extract features from each document.<br>\n",
    "\n",
    "**This is for illustration only and to help your understand of word vectorization!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentence1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentence1.txt\n",
    "I really like this NLP module now!\n",
    "Letterkenny is a great town.\n",
    "I would like to go for a coffee now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sentence2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentence2.txt\n",
    "I like to go shopping and to have a coffee.\n",
    "Is it far to the town from here?\n",
    "NLP is not complicated in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a vocabulary\n",
    "\n",
    "In NLP, we always use a vocabulary. Even when we load spacy, we also load a dictionary file in a specific language.\n",
    "\n",
    "In the next Python cide, I'm building a numerical array from all the words that appear in both text documents. Then I'll create instances (vectors) for each individual document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'really': 2, 'like': 3, 'this': 4, 'nlp': 5, 'module': 6, 'now!': 7, 'letterkenny': 8, 'is': 9, 'a': 10, 'great': 11, 'town.': 12, 'would': 13, 'to': 14, 'go': 15, 'for': 16, 'coffee': 17, 'now.': 18}\n"
     ]
    }
   ],
   "source": [
    "vocab_dictionary = {}\n",
    "counter = 1\n",
    "\n",
    "with open('sentence1.txt') as file:\n",
    "    sentence_file = file.read().lower().split()\n",
    "\n",
    "for word in sentence_file:\n",
    "    # if the word is already present then skip\n",
    "    if word in vocab_dictionary:\n",
    "        continue\n",
    "    # If the word doesnt exists in the dictionary, add it with a \n",
    "    # uniques counter (index) number\n",
    "    else:\n",
    "        vocab_dictionary[word]=counter\n",
    "        counter+=1\n",
    "\n",
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this step for the second sentence. Add new unseen words to the end of the dictionary and assign a unique identifier to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'really': 2, 'like': 3, 'this': 4, 'nlp': 5, 'module': 6, 'now!': 7, 'letterkenny': 8, 'is': 9, 'a': 10, 'great': 11, 'town.': 12, 'would': 13, 'to': 14, 'go': 15, 'for': 16, 'coffee': 17, 'now.': 18, 'shopping': 19, 'and': 20, 'have': 21, 'coffee.': 22, 'it': 23, 'far': 24, 'the': 25, 'town': 26, 'from': 27, 'here?': 28, 'not': 29, 'complicated': 30, 'in': 31, 'module.': 32}\n"
     ]
    }
   ],
   "source": [
    "with open('sentence2.txt') as file:\n",
    "    sentence_file = file.read().lower().split()\n",
    "\n",
    "for word in sentence_file:\n",
    "    if word in vocab_dictionary:\n",
    "        continue\n",
    "    else:\n",
    "        vocab_dictionary[word]=counter\n",
    "        counter+=1\n",
    "\n",
    "print(vocab_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are 25 words in **sentence2** file, there were only 14 additional words added to the vocabulary dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Now that we've encapsulated our \"entire language\" in a dictionary, we can perform *feature extraction* on each of our original documents:\n",
    "\n",
    "First we create empty vectors for spaces for each word in the vocabulary dictionary. It contains 32 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence 1', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['sentence 2', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "one = ['sentence 1']+[0]*len(vocab_dictionary)\n",
    "print(one)\n",
    "\n",
    "two = ['sentence 2']+[0]*len(vocab_dictionary)\n",
    "print(two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I map the frequency of each word in the **sentence1.txt** file into the **sentence one** vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence 1', 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# map the frequencies of each word in 1.txt to our vector:\n",
    "with open('sentence1.txt') as file:\n",
    "    sentence_file = file.read().lower().split()\n",
    "    \n",
    "for word in sentence_file:\n",
    "    one[vocab_dictionary[word]]+=1\n",
    "    \n",
    "print(one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this process is repeated for **sentence 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence 2', 1, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 3, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Do the same for the second document:\n",
    "with open('sentence2.txt') as file:\n",
    "    sentence_file = file.read().lower().split()\n",
    "    \n",
    "for word in sentence_file:\n",
    "    two[vocab_dictionary[word]]+=1\n",
    "    \n",
    "print(two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with vecotrs that contain some frequency counts of words for each text file, as well as a lot of elements where there is no representation of words in the text file, as represented by **0**. This is called a **sparse matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence 1', 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['sentence 2', 1, 0, 1, 1, 1, 0, 0, 0, 2, 1, 0, 0, 0, 3, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Compare the two vectors:\n",
    "print(f'{one}\\n{two}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the vectors we see that some words are common to both, some appear only in sentence1.txt, others only in sentence2.txt. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words and Tf-idf\n",
    "\n",
    "Bag of Words (BoW) is a model used in NLP. One aim of BoW is to categorise documents. The idea is to analyse and classify different **bags of words** (corpus). By matching the different categories, we identify which “bag” a certain block of text comes from. For example, in our spam filtering example, the system is trained to differentiate between **spam** and **ham**. We are trying to get our system to identify which bag the document comes from, the **bag of spam** or the **bag of ham**.\n",
    "\n",
    "In the examples discussed in the lecture notes **Text feature extraction** on Blackboard, each vector can be considered a bow. By itself these may not be helpful until we consider **term frequencies**, or how often individual words appear in documents. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n",
    "\n",
    "However, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider **inverse document frequency**, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency).\n",
    "\n",
    "Together these terms become [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words and Word Stems\n",
    "Some common words such as **the** and **and** appear so frequently in text that we can't use them to accurately reflect the contents of words in documents. \n",
    "\n",
    "It may only make sense to record the root of a word, for example **house** instead of **houses**. This helps to shrink the required space for our vocab array and improve overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Tagging\n",
    "\n",
    "When we created our vectors the first thing we did was split the incoming text on whitespace with `.split()`. This was a crude form of **tokenization** - that is, dividing a document into individual words or tokens.\n",
    "\n",
    "Once the text is divided, we can go back and **tag** our tokens with information about **parts of speech**, **grammatical dependencies**, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become ***high dimensional sparse matrices***."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
