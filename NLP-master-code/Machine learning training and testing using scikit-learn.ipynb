{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning training, testing and evaluating\n",
    "\n",
    "**Scikit-learn** is an open-source machine learning library for Python that offers a variety of regression, classification and clustering algorithms. You can find a lot of detailed information on it at this link http://scikit-learn.org\n",
    "\n",
    "In this section we'll continue from the previous session whereby I added some extra columns into the data frame to see if it helps us to predict whether a text message is **ham** or **spam**. We also checked for missing values. The code below is a summary of that in the previous lecture. Run all this code to build the dataset with new variables in it.\n",
    "\n",
    "In this lecture we'll perform a fairly simple classification exercise with scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tsv file into a dataframe object\n",
    "# Press tab to check you are in the correct folder location and to browse\n",
    "# to the tsv file\n",
    "# The sep command indicates this files is separated by tabs\n",
    "dataframe = pd.read_csv(\"../Jupyter notebook files/SMSSpamCollection/SMSSpamCollection.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the methods we could use to determine whether a text message is **HAM** or **SPAM** is through examination of the length of characters in each line of text.\n",
    "\n",
    "Let's create a loop that uses a list to contain the number of characters within each line. then we'll add the lenght to the end of each row in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_length_col = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    length_message_text = len(row.message)\n",
    "    # add the length of each message to list\n",
    "    message_length_col.append(length_message_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll add this list to the dataframe. I'm also inserting this data under the column heading **length**.\n",
    "\n",
    "See this link for further information \n",
    "https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll add the contents of this list to a new column\n",
    "# called \"length\" to the end of our dataframe\n",
    "# See https://www.geeksforgeeks.org/adding-new-column-to-existing-dataframe-in-pandas/\n",
    "dataframe['length'] = message_length_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_data = []\n",
    "spam_data = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    # If the label data is recognised to be \"ham\"\n",
    "    if row[\"label\"] == \"ham\":\n",
    "        ham_data.append(row)\n",
    "    else:\n",
    "        spam_data.append(row)\n",
    "\n",
    "# Convert list to a dataframe before performing descriptive statistics on it\n",
    "ham_dataframe = pd.DataFrame(ham_data)\n",
    "spam_dataframe = pd.DataFrame(spam_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables first\n",
    "punct_length_col = []\n",
    "punct_count = 0\n",
    "\n",
    "for index, textrow in dataframe.iterrows():\n",
    "    doc_object = nlp(textrow.message)\n",
    "    for word in doc_object:\n",
    "        if word.pos_ == 'PUNCT':           \n",
    "            punct_count += 1\n",
    "    # Sentence is checked so add count to list\n",
    "    punct_length_col.append(punct_count)\n",
    "    punct_count =0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to ensure that the list we're going to insert into the text dataframe contaisn the same number of rows. Otherwise we'll get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add punct list to dataframe\n",
    "dataframe['punct'] = punct_length_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      4\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      2\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      1\n",
       "3   ham  U dun say so early hor... U c already then say...      49      2\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View top of dataframe content\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be enough difference between text length in **SPAM** texts versus **HAM** ones to uniquely identify one against the other, but there is not as distinct difference between the number of punctuations in **HAM** messages compared to **SPAM**.\n",
    "\n",
    "We'll now create a ML model using `scikit learn`. In the next lecture we'll use the text content to build a more accurate ML model.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning model creation through scikit-learn\n",
    "\n",
    "Every algorithm is accessed in `scikit-learn` using an **estimator**.\n",
    "\n",
    "The genral syntax to import a model is:\n",
    "\n",
    ">`from sklearn.family import Model`\n",
    "\n",
    "For example, to use the `linear regression` model, we would use:\n",
    "\n",
    ">`from sklearn.linear_model import LinearRegression`.\n",
    "\n",
    "We can set all of the **estimator** parameters when it is instantiated. If we don't, suitable default values will be applied to the ML model. We can press `SHIFT + TAB` in Jupyter notebook to view all of the possible parameters for each model.\n",
    "<br>\n",
    "For example, if we were creating a Linear regression model, and we wanted it to be normalised, we would set the `normalize` parameter to `True`. We can view all of the parameters of a ML model by using the `print(model)` command.\n",
    "\n",
    "Once the model is created, then we need to fit the model with data. As described in an earlier lecture, the data is split into **training** and **testing** data. We'll work through this process in the upcoming code.\n",
    "\n",
    "Once the data is split, then we can fit (or train) or model on the **training** data using the `model.fit()` command. The syntax is :\n",
    "\n",
    ">`model.fit(X_train, y_train)`\n",
    "\n",
    "Note that I'm using specific syntax to do this. Refer to the earlier lecture on supervised learnign for further information. \n",
    "\n",
    "Once the model has been fit and trained on the training data, it is then ready for prediction on the test dataset. We predict data from the trained model using this command:\n",
    "\n",
    ">`predict = model.predict(X_test)`\n",
    "\n",
    "We then evaluate the ML model by comparing predicted value to actual test values. With **classification** models, we will examine accuracy, F1 score etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and testing datasets\n",
    "\n",
    "Before we instantiate our ML model, we'll divide the dataset into 2 smaller training and testing datasets.\n",
    "If we want to divide the DataFrame into two smaller sets, we could use\n",
    "> `train, test = train_test_split(dataframe)`\n",
    "\n",
    "We'll also set up our Features (X) and Labels (y). The Label is simple - we're trying to predict the `label` column in our data. For Features we'll use the `length` and `punct` columns. \n",
    "\n",
    "*Please note, **X** is capitalised and **y** is lowercase.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the feature data\n",
    "# We're creating a list of column names to\n",
    "# use from our dataframe.\n",
    "# We need 2 brackets as there is more than 1 entry\n",
    "X = dataframe[['length', 'punct']]\n",
    "\n",
    "# This is the label data - 1 entry\n",
    "# so only need 1 set of brackets\n",
    "y = dataframe['label']\n",
    "\n",
    "# Use SHIFT + TAB to see full options and to\n",
    "# copy some contents below\n",
    "# test-size represents percentage to use for testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine the size of the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train data shape (3900, 2)\n",
      "X test data shape (1672, 2)\n"
     ]
    }
   ],
   "source": [
    "# Contains 2 columns\n",
    "print('X train data shape', X_train.shape)\n",
    "print('X test data shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train data shape (3900,)\n",
      "y test data shape (1672,)\n"
     ]
    }
   ],
   "source": [
    "# 1 column of label data\n",
    "print('y train data shape', y_train.shape)\n",
    "print('y test data shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4393     ham\n",
       "216      ham\n",
       "4471     ham\n",
       "3889     ham\n",
       "5030    spam\n",
       "2000     ham\n",
       "4135     ham\n",
       "1055     ham\n",
       "3646    spam\n",
       "4774     ham\n",
       "1758     ham\n",
       "4796     ham\n",
       "4857     ham\n",
       "5311     ham\n",
       "2879    spam\n",
       "1416     ham\n",
       "3680     ham\n",
       "460      ham\n",
       "3431     ham\n",
       "1784     ham\n",
       "4570     ham\n",
       "1135     ham\n",
       "267      ham\n",
       "1892     ham\n",
       "1926     ham\n",
       "3853     ham\n",
       "3663     ham\n",
       "1521    spam\n",
       "3225     ham\n",
       "4583     ham\n",
       "        ... \n",
       "1031     ham\n",
       "1110     ham\n",
       "1888    spam\n",
       "3550     ham\n",
       "1527     ham\n",
       "753      ham\n",
       "3049     ham\n",
       "2628     ham\n",
       "562      ham\n",
       "4764     ham\n",
       "3562    spam\n",
       "252      ham\n",
       "2516     ham\n",
       "2962     ham\n",
       "4453     ham\n",
       "5374     ham\n",
       "5396     ham\n",
       "1202     ham\n",
       "3462     ham\n",
       "2797     ham\n",
       "4225     ham\n",
       "144      ham\n",
       "5056     ham\n",
       "2895     ham\n",
       "2763     ham\n",
       "905      ham\n",
       "5192     ham\n",
       "3980     ham\n",
       "235     spam\n",
       "5157     ham\n",
       "Name: label, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index position matches with index\n",
    "# position in X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      length  punct\n",
      "1078      28      1\n",
      "4028      45      3\n",
      "958       26      0\n",
      "4642       7      1\n",
      "4674     107      5\n",
      "5461      51      3\n",
      "4210      74      5\n",
      "4216      26      1\n",
      "1603      25      2\n",
      "1504      31      2\n",
      "1783      53      2\n",
      "3465       8      0\n",
      "5534      28      0\n",
      "4267      85      1\n",
      "2498      51      2\n",
      "4259      29      1\n",
      "147      159      4\n",
      "141       33      1\n",
      "4517     161     10\n",
      "3053      49      2\n",
      "5392      59      0\n",
      "2346      41      2\n",
      "1242      59      2\n",
      "3224      33      0\n",
      "4872      35      2\n",
      "3044      42      1\n",
      "1660      28      2\n",
      "3214      14      1\n",
      "501      149      6\n",
      "1827     332     11\n",
      "...      ...    ...\n",
      "1673     157      4\n",
      "1433      38      1\n",
      "616      145      2\n",
      "3416      29      0\n",
      "4035      54      3\n",
      "1646      31      1\n",
      "1395      36      1\n",
      "630      148      2\n",
      "955       41      0\n",
      "194      111      2\n",
      "4392      85      1\n",
      "909       31      0\n",
      "5540     158      5\n",
      "1006      37      0\n",
      "5080      93      3\n",
      "4548     124      6\n",
      "5345      16      1\n",
      "4545      35      1\n",
      "368      129      3\n",
      "3677      53      2\n",
      "4692      51      2\n",
      "3531      87      4\n",
      "3409     157      1\n",
      "4964      34      1\n",
      "2332      18      0\n",
      "3954     114      6\n",
      "619       59      2\n",
      "1987      24      1\n",
      "2358      52      1\n",
      "3594      22      1\n",
      "\n",
      "[1672 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078     ham\n",
       "4028     ham\n",
       "958      ham\n",
       "4642     ham\n",
       "4674     ham\n",
       "5461     ham\n",
       "4210     ham\n",
       "4216     ham\n",
       "1603     ham\n",
       "1504     ham\n",
       "1783     ham\n",
       "3465     ham\n",
       "5534     ham\n",
       "4267     ham\n",
       "2498     ham\n",
       "4259     ham\n",
       "147     spam\n",
       "141      ham\n",
       "4517    spam\n",
       "3053     ham\n",
       "5392     ham\n",
       "2346     ham\n",
       "1242     ham\n",
       "3224     ham\n",
       "4872     ham\n",
       "3044     ham\n",
       "1660     ham\n",
       "3214     ham\n",
       "501      ham\n",
       "1827     ham\n",
       "        ... \n",
       "1673    spam\n",
       "1433     ham\n",
       "616      ham\n",
       "3416     ham\n",
       "4035     ham\n",
       "1646     ham\n",
       "1395     ham\n",
       "630     spam\n",
       "955     spam\n",
       "194      ham\n",
       "4392     ham\n",
       "909      ham\n",
       "5540    spam\n",
       "1006     ham\n",
       "5080     ham\n",
       "4548     ham\n",
       "5345     ham\n",
       "4545     ham\n",
       "368     spam\n",
       "3677     ham\n",
       "4692     ham\n",
       "3531     ham\n",
       "3409    spam\n",
       "4964     ham\n",
       "2332     ham\n",
       "3954    spam\n",
       "619      ham\n",
       "1987     ham\n",
       "2358     ham\n",
       "3594     ham\n",
       "Name: label, Length: 1672, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers with our data\n",
    "Now that we have our training and testing datasets created, we can use this data to build classification models. We'll build several models and see how each one differs according to accuracy.\n",
    "\n",
    "There are several classifiers that we can use for our text datasets. Data can more easily be separated linearly with classifiers such as naive Bayes and linear SVMs, and might lead to better generalisation than is achieved by other classifiers. See https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html?highlight=classifiers for more information on classifiers.\n",
    "\n",
    "See https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/ for more information on various classifiers that are available to us.\n",
    "\n",
    "There are 4 classifiers that we are going to train and test, and then examien for overall accuracy. There's more than just these models, but these are  particularly suited to sentiment analysis and text classification.\n",
    "\n",
    "(a) Linear classifier<br> \n",
    "(b) naïve Bayes<br> \n",
    "(c) Random Forest<br> \n",
    "(d) Support Vector Classifier<br> \n",
    "\n",
    "**Note** - as with all classifiers, we need to evaluate overall accuracy of the model after it is built. The theory conveyed by some classifiers does not necessarily carry over to real datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Logistic Regression classifier (model)\n",
    "\n",
    "One of the simplest multi-class classification tools is [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "The following steps are used with all classifiers, not just logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - import the model we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - create an instance of the model we imported\n",
    "\n",
    "Next we create a specific instance of the model we want to construct. Note that I'm using the same model that I imported in Step 1.\n",
    "\n",
    "There are lots of different settings available for the model. To see these settings, press the `SHIFT + TAB` keys **twice** when the cursor is over the line of code. Then scroll down through the settings window. In this example we are going to use the **L-BFGS** algorithmic solver. See this link for further information on the various parameters available for the logistic regression model https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "\n",
    "The default values are usually good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - build the model and fit data to it\n",
    "\n",
    "We build the model with any specifc options we want to set that are not the dafult values. In this example, I'm using the **L-BFGS** option.\n",
    "\n",
    "Once the model is built, I provide training data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_model = LogisticRegression(solver='lbfgs')\n",
    "# Note that the \"fit\" option must be run in the same cell as line above\n",
    "lin_reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model accuracy\n",
    "\n",
    "Now we are going to test the accurcy of the model using the test data.\n",
    "\n",
    "Firstly we import the `sklearn.metrics` module which includes score functions, performance metrics and pairwise metrics and distance computations. See https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics for more information. \n",
    "\n",
    "Now we create a **predictions** set with some test data. The model has not yet seen this data. It contains **length** amd **punctuation** data for each text message that we already have correct answer for in the **y_test** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Create a prediction set:\n",
    "# The model has not yet seen contents of X_test\n",
    "# which is a dataset of message length and punctuation\n",
    "# And we know to expect answers in y_test\n",
    "# which is a list of expected label output\n",
    "lin_reg_model_predictions = lin_reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the contents of the **predictions** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the predicted output from the model\n",
    "lin_reg_model_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metrics` model contains a `confusion metrics` option that provides us with options to build a confusion matrix to evaluate model accuracy. \n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1391   51]\n",
      " [ 220   10]]\n"
     ]
    }
   ],
   "source": [
    "# Now we compare what the model predicted \n",
    "# with what is expected as output\n",
    "# Print a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,lin_reg_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of confusions in this model is 271.\n",
    "\n",
    "We can create a dataframe and add labels to make the confusion matrix easier to read and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted ham</th>\n",
       "      <th>predicted spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct ham</th>\n",
       "      <td>1391</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct spam</th>\n",
       "      <td>220</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              predicted ham  predicted spam\n",
       "correct ham            1391              51\n",
       "correct spam            220              10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can make the confusion matrix less confusing by adding labels:\n",
    "dataframe_labels = pd.DataFrame(metrics.confusion_matrix(y_test,lin_reg_model_predictions), \n",
    "                  index=['correct ham','correct spam'], \n",
    "                  columns=['predicted ham','predicted spam'])\n",
    "dataframe_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix we can see that the model correctly classified 10 spam messages, and incorrectly classified 51 ham messages as spam.\n",
    "\n",
    "The model is better at correctly classifying ham with 1391 correclty classified, and 51 ham messages were incorrectly classified as spam. But the results are terrible.\n",
    "\n",
    "Accuracy = TP + TN / Total\n",
    "= 1391 + 51 / 1672\n",
    "= 1442/1672 = 0.86\n",
    "\n",
    "Let's look at a classification report to show precison, recall and F1-score.\n",
    "\n",
    "Overall the model is good at predicting **ham**, and not so good at **spam**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.86      0.96      0.91      1442\n",
      "        spam       0.16      0.04      0.07       230\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1672\n",
      "   macro avg       0.51      0.50      0.49      1672\n",
      "weighted avg       0.77      0.84      0.80      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,lin_reg_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show the overall accuracy of the model with this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8379186602870813\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,lin_reg_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy value indicates that the overall accuray of the model is less than if the model were just to predict **HAM** for all messages in the test dataset (0.86).\n",
    "\n",
    "Let's evaluate other models that are available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a naïve Bayes classifier\n",
    "\n",
    "One of the most common classifiers is naive Bayes model .See http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes). for more information. It is particularly good for document classification and spam filtering. \n",
    "\n",
    "We will build this model using the same steps we used earlier. We'll import the model from scikit-learn, create an instance of the model, fit (train) the model using our training data, and then predict data using the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import the model we want to use\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create an instance of the model - common model for text data and spam filtering\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Fit model to training data\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1442    0]\n",
      " [ 230    0]]\n"
     ]
    }
   ],
   "source": [
    "# Predict answers to data from the X_text dataset\n",
    "# containing text length and punctuation count\n",
    "nb_model_predictions = nb_model.predict(X_test)\n",
    "\n",
    "# Show results in a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,nb_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall confusion is 230, down from 271 for the logistic regression classifier.\n",
    "\n",
    "Now we can see that this model is no longer any good at predicting spam at all in our text messages.\n",
    "\n",
    "We can look at this in more detail with the `sklearn metrics` report. The warning from scikit-learn shows us that the model cannot predict spam within the text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda3\\envs\\ai2_course\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.86      1.00      0.93      1442\n",
      "        spam       0.00      0.00      0.00       230\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1672\n",
      "   macro avg       0.43      0.50      0.46      1672\n",
      "weighted avg       0.74      0.86      0.80      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,nb_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can view the overall accuracy of the model. Note how the overall accuracy appears to suggest that the model is quite accurate, but it is no better than one where only ham is provided as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8624401913875598\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,nb_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model\n",
    "\n",
    "A random forest is a bagging model, and part of the tree model family. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build this model using the same steps we used earlier. We'll import the model from scikit-learn, create an instance of the model, fit (train) the model using our training data, and then predict data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\Anaconda3\\envs\\ai2_course\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First import the model we want to use\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the model - common model for text data and spam filtering\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Fit model to training data\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1360   82]\n",
      " [ 108  122]]\n"
     ]
    }
   ],
   "source": [
    "# Predict answers to data from the X_text dataset\n",
    "# containing text length and punctuation count\n",
    "rf_model_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Show results in a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall confusion is 200 (120 + 80), down from 230 for the naïve Bayes classifier.\n",
    "\n",
    "Now we can see that this model is better than naïve Bayes classifier at predicting spam in our text messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.93      0.94      0.93      1442\n",
      "        spam       0.60      0.53      0.56       230\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1672\n",
      "   macro avg       0.76      0.74      0.75      1672\n",
      "weighted avg       0.88      0.89      0.88      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that this model is very good at predicting HAM text messages, amd better than the other 2 models when it comes to predicting SPAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8863636363636364\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,rf_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall accuracy of the model suggest that it is better than logistic regression (0.84) and naïve Bayes (0.86).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Support Vector Classifier (SVC)\n",
    "\n",
    "Lets examine whether a SVM will improve the model accuracy.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for further information on scikit-learn SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a Support Vector Classification model (SVC)\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1373   69]\n",
      " [ 112  118]]\n"
     ]
    }
   ],
   "source": [
    "# Setting gamma to \"auto\", otherwise the SVC model \n",
    "# returns an error\n",
    "svc_model = SVC(gamma=\"auto\")\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "svc_model_predictions = svc_model.predict(X_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, svc_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall confusion is (112 + 69) = 181. This is better than naive Bayes model (230), the Random Forst model (200) and the Logistic Regression classifier (271).\n",
    "\n",
    "And we can examine the metrics report for further detail on overall accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      0.95      0.94      1442\n",
      "        spam       0.63      0.51      0.57       230\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      1672\n",
      "   macro avg       0.78      0.73      0.75      1672\n",
      "weighted avg       0.88      0.89      0.89      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,svc_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that this model is very good at predicting **HAM** text messages, amd better than the other 2 models when it comes to predicting **SPAM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8917464114832536\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test,svc_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better than the naive Bayes model (0.86 overall accuracy) and the Logistic Regression classifier (0.83 overall accuracy) and slightly better than the Random Forest model (0.88)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
